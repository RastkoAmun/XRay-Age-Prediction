{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers.utils import CustomImageDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('data/boneage-training-dataset.csv')\n",
    "training_labels, testing_labels = train_test_split(labels_df, train_size=0.95, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  11980\n",
      "Test size:  631\n"
     ]
    }
   ],
   "source": [
    "transformer = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Using custom dataset to load images \n",
    "training_dataset = CustomImageDataset(\n",
    "  root_dir='data/processed/training-set', labels=training_labels, transform=transformer\n",
    ")\n",
    "testing_dataset = CustomImageDataset(\n",
    "  root_dir='data/processed/training-set', labels=testing_labels, transform=transformer\n",
    ")\n",
    "\n",
    "print(\"Training size: \", len(training_dataset))\n",
    "print(\"Test size: \", len(testing_dataset))\n",
    "batch_size = 32\n",
    "\n",
    "# prepared dataloader for neural network (note it is using batch size of 3, just for this sample)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoneAgeModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BoneAgeModel, self).__init__()\n",
    "\n",
    "    self.cnn = nn.Sequential(\n",
    "      #First conv block\n",
    "      nn.Conv2d(1, 32, kernel_size=3, padding=1),\\\n",
    "      nn.BatchNorm2d(32), # Normalize \n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "      #Second conv block \n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(64), \n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "      #Third conv block \n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(128), \n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "      #Forth conv block\n",
    "      nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(256), \n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    )\n",
    "    feature_size = 256 * 16 * 21\n",
    "\n",
    "    self.fc_layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "          \n",
    "      nn.Linear(feature_size, 512),  \n",
    "      nn.BatchNorm1d(512),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.3), \n",
    "            \n",
    "      nn.Linear(512, 256),\n",
    "      nn.BatchNorm1d(256),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.3), \n",
    "            \n",
    "      nn.Linear(256, 1) \n",
    "    ) \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.cnn(x)\n",
    "    x = self.fc_layers(x)\n",
    "  \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, training_dataloader, testing_dataloader, num_epochs):\n",
    "   # Use both MSE and MAE\n",
    "    criterion_mse = nn.MSELoss() # MSE penalizes larger error more severely \n",
    "    criterion_mae = nn.L1Loss() \n",
    "\n",
    "    #Adam optimizer with weigth decay (L2)\n",
    "    #https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "    optimizer  = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor = 0.5, patience=3\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None \n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, ages, _, _ in training_dataloader:\n",
    "            images = images.float()\n",
    "            ages = ages.float()\n",
    "\n",
    "            optimizer.zero_grad() # clear gradients from the previous batch \n",
    "\n",
    "            #forward pass \n",
    "            predictions = model(images).squeeze()\n",
    "            \n",
    "            loss_mse = criterion_mse(predictions, ages)\n",
    "            loss_mae = criterion_mae(predictions, ages)\n",
    "            loss = loss_mse + loss_mae\n",
    "\n",
    "            # Bakcward pass \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(training_dataloader) \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        #validation phase \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        mae_total = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, ages, _, _ in testing_dataloader:\n",
    "                images = images.float()\n",
    "                ages = ages.float()\n",
    "\n",
    "                # Forward pass \n",
    "                prediction = model(images).squeeze()\n",
    "                \n",
    "                mae = criterion_mae(prediction, ages) \n",
    "                val_loss += mae.item()\n",
    "\n",
    "                mae_total += torch.abs(prediction - ages).sum().item() \n",
    "\n",
    "        val_loss /= len(testing_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        mae_months = mae_total / len(testing_dataloader.dataset)\n",
    "        val_maes.append(mae_months)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}, MAE: {mae_months:.2f} months\")\n",
    "\n",
    "        #Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss: \n",
    "            best_val_loss = val_loss\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            print(f\"New best model saved!\")\n",
    "    \n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoneAgeModel()\n",
    "model = train_model(model, training_dataloader, testing_dataloader, num_epochs=5)\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "epoch_val_loss = 0\n",
    "mae_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, ages, _,img_ids in testing_dataloader:\n",
    "        images = images.float()\n",
    "        ages = ages.float()\n",
    "\n",
    "        predictions = model(images).squeeze()\n",
    "\n",
    "        for i in range(len(img_ids)):\n",
    "            test_predictions.append([img_ids[i].item(), predictions[i].item()])\n",
    "\n",
    "            loss = torch.abs(predictions[i] - ages[i])\n",
    "            epoch_val_loss += loss.item()\n",
    "            mae_total += loss.item()\n",
    "\n",
    "            print(f\"{img_ids[i].item():>6} {ages[i].item():>8} {predictions[i].item():>8.2f}\")\n",
    "\n",
    "val_loss = epoch_val_loss / len(testing_dataloader.dataset)\n",
    "mae_months = mae_total / len(testing_dataloader.dataset)\n",
    "print(f\"\\n Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae_months:.2f} months\")\n",
    "\n",
    "submission_df = pd.DataFrame(test_predictions, columns=[\"id\", \"bone_age\"])  \n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_losses, val_losses, val_maes):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Losses plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # MAE plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_maes, label='Validation MAE', marker='o', color='g')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE (months)')\n",
    "    plt.title('Validation MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
