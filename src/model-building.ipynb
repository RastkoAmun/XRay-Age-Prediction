{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers.utils import CustomImageDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('data/boneage-training-dataset.csv')\n",
    "training_labels, testing_labels = train_test_split(labels_df, train_size=0.95, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  11980\n",
      "Test size:  631\n"
     ]
    }
   ],
   "source": [
    "transformer = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Using custom dataset to load images \n",
    "training_dataset = CustomImageDataset(\n",
    "  root_dir='data/processed/training-set', labels=training_labels, transform=transformer\n",
    ")\n",
    "testing_dataset = CustomImageDataset(\n",
    "  root_dir='data/processed/training-set', labels=testing_labels, transform=transformer\n",
    ")\n",
    "\n",
    "print(\"Training size: \", len(training_dataset))\n",
    "print(\"Test size: \", len(testing_dataset))\n",
    "batch_size = 32\n",
    "\n",
    "# prepared dataloader for neural network (note it is using batch size of 3, just for this sample)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoneAgeModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BoneAgeModel, self).__init__()\n",
    "\n",
    "    self.cnn = nn.Sequential(\n",
    "      #First conv block\n",
    "      nn.Conv2d(1, 32, kernel_size=3, padding=1),\\\n",
    "      nn.BatchNorm2d(32), # Normalize \n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "      #Second conv block \n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(64), \n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "      #Third conv block \n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(128), \n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "      #Forth conv block\n",
    "      nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(256), \n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    )\n",
    "    feature_size = 256 * 16 * 21\n",
    "\n",
    "    self.fc_layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "          \n",
    "      nn.Linear(feature_size, 512),  \n",
    "      nn.BatchNorm1d(512),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.3), \n",
    "            \n",
    "      nn.Linear(512, 256),\n",
    "      nn.BatchNorm1d(256),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.3), \n",
    "            \n",
    "      nn.Linear(256, 1) \n",
    "    ) \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.cnn(x)\n",
    "    x = self.fc_layers(x)\n",
    "  \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, training_dataloader, testing_dataloader, num_epochs):\n",
    "   # Use both MSE and MAE\n",
    "    criterion_mse = nn.MSELoss() # MSE penalizes larger error more severely \n",
    "    criterion_mae = nn.L1Loss() \n",
    "\n",
    "    #Adam optimizer with weigth decay (L2)\n",
    "    #https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "    optimizer  = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor = 0.5, patience=3\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None \n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, ages, _, _ in training_dataloader:\n",
    "            images = images.float()\n",
    "            ages = ages.float()\n",
    "\n",
    "            optimizer.zero_grad() # clear gradients from the previous batch \n",
    "\n",
    "            #forward pass \n",
    "            predictions = model(images).squeeze()\n",
    "            \n",
    "            loss_mse = criterion_mse(predictions, ages)\n",
    "            loss_mae = criterion_mae(predictions, ages)\n",
    "            loss = loss_mse + loss_mae\n",
    "\n",
    "            # Bakcward pass \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(training_dataloader) \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        #validation phase \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        mae_total = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, ages, _, _ in testing_dataloader:\n",
    "                images = images.float()\n",
    "                ages = ages.float()\n",
    "\n",
    "                # Forward pass \n",
    "                prediction = model(images).squeeze()\n",
    "                \n",
    "                mae = criterion_mae(prediction, ages) \n",
    "                val_loss += mae.item()\n",
    "\n",
    "                mae_total += torch.abs(prediction - ages).sum().item() \n",
    "\n",
    "        val_loss /= len(testing_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        mae_months = mae_total / len(testing_dataloader.dataset)\n",
    "        val_maes.append(mae_months)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}, MAE: {mae_months:.2f} months\")\n",
    "\n",
    "        #Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss: \n",
    "            best_val_loss = val_loss\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            print(f\"New best model saved!\")\n",
    "    \n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 10296.15, Val Loss: 40.19, MAE: 40.20 months\n",
      "New best model saved!\n",
      "Epoch 2/20, Train Loss: 1278.15, Val Loss: 22.12, MAE: 22.12 months\n",
      "New best model saved!\n",
      "Epoch 3/20, Train Loss: 723.66, Val Loss: 36.97, MAE: 36.96 months\n",
      "Epoch 4/20, Train Loss: 594.33, Val Loss: 45.87, MAE: 45.92 months\n",
      "Epoch 5/20, Train Loss: 505.05, Val Loss: 20.79, MAE: 20.81 months\n",
      "New best model saved!\n",
      "Epoch 6/20, Train Loss: 467.23, Val Loss: 15.50, MAE: 15.50 months\n",
      "New best model saved!\n",
      "Epoch 7/20, Train Loss: 422.59, Val Loss: 28.83, MAE: 28.89 months\n",
      "Epoch 8/20, Train Loss: 392.74, Val Loss: 25.96, MAE: 25.97 months\n",
      "Epoch 9/20, Train Loss: 353.14, Val Loss: 19.11, MAE: 19.14 months\n",
      "Epoch 10/20, Train Loss: 330.56, Val Loss: 17.42, MAE: 17.47 months\n",
      "Epoch 11/20, Train Loss: 272.55, Val Loss: 17.04, MAE: 17.06 months\n",
      "Epoch 12/20, Train Loss: 256.85, Val Loss: 14.83, MAE: 14.84 months\n",
      "New best model saved!\n",
      "Epoch 13/20, Train Loss: 230.92, Val Loss: 14.42, MAE: 14.41 months\n",
      "New best model saved!\n",
      "Epoch 14/20, Train Loss: 225.53, Val Loss: 14.23, MAE: 14.24 months\n",
      "New best model saved!\n",
      "Epoch 15/20, Train Loss: 224.78, Val Loss: 16.77, MAE: 16.78 months\n",
      "Epoch 16/20, Train Loss: 213.48, Val Loss: 13.77, MAE: 13.81 months\n",
      "New best model saved!\n",
      "Epoch 17/20, Train Loss: 210.15, Val Loss: 13.83, MAE: 13.84 months\n",
      "Epoch 18/20, Train Loss: 201.44, Val Loss: 13.57, MAE: 13.60 months\n",
      "New best model saved!\n",
      "Epoch 19/20, Train Loss: 201.01, Val Loss: 14.51, MAE: 14.52 months\n",
      "Epoch 20/20, Train Loss: 187.20, Val Loss: 13.53, MAE: 13.53 months\n",
      "New best model saved!\n",
      " 12427    156.0   167.99\n",
      "  8304    150.0   137.18\n",
      "  6046     82.0   123.85\n",
      "  9337    132.0   129.85\n",
      " 15447    132.0   150.57\n",
      " 13951    126.0   140.21\n",
      " 13853    168.0   183.99\n",
      "  8063    180.0   180.35\n",
      "  2984    162.0   148.09\n",
      "  7901    156.0   158.78\n",
      "  2517    192.0   183.66\n",
      "  8989    106.0   112.65\n",
      " 13662     94.0   101.86\n",
      " 11825    120.0   140.19\n",
      " 15104     18.0    27.69\n",
      " 14952    106.0   120.96\n",
      "  9700    126.0   159.40\n",
      "  6526    138.0   158.19\n",
      " 14696    144.0   159.63\n",
      " 13639    144.0   160.46\n",
      "  5902    132.0   144.13\n",
      "  2662    125.0    83.96\n",
      "  9878    102.0   108.44\n",
      " 13269    156.0   155.65\n",
      "  6973    138.0   152.88\n",
      " 12320    186.0   189.57\n",
      "  1730    113.0   136.11\n",
      " 14735    132.0   135.78\n",
      "  5160    132.0   158.76\n",
      " 11267    144.0   164.16\n",
      " 10826    120.0   135.26\n",
      " 13798     50.0    66.72\n",
      " 15579     96.0   110.08\n",
      " 13344    156.0   143.40\n",
      "  4676    100.0   119.79\n",
      "  2174    150.0   160.16\n",
      " 13048     82.0    95.75\n",
      "  5823    150.0   158.75\n",
      "  5348    138.0   130.92\n",
      "  2781    162.0   153.58\n",
      "  9972    150.0   130.73\n",
      " 14100    162.0   156.07\n",
      "  3483     62.0   119.22\n",
      " 13219    132.0   114.37\n",
      " 11700    138.0   128.36\n",
      "  6045    144.0   151.72\n",
      "  5718    144.0   137.91\n",
      " 14191    106.0   116.78\n",
      " 12710    132.0   109.91\n",
      " 10577     94.0    91.35\n",
      "  5149     36.0    56.51\n",
      " 12476    138.0   128.38\n",
      "  6937    150.0   114.56\n",
      "  3837    138.0   137.71\n",
      " 11453    168.0   160.05\n",
      "  9976     30.0    52.70\n",
      "  6622    113.0   135.77\n",
      "  2404    144.0   158.77\n",
      "  1459     84.0   165.13\n",
      "  9256    120.0   129.69\n",
      " 10395    168.0   152.41\n",
      "  6160     72.0    62.31\n",
      "  6258     66.0    71.90\n",
      " 11614     82.0    82.60\n",
      "  9156    150.0   153.11\n",
      "  5825     51.0    47.06\n",
      " 12078    132.0   105.28\n",
      "  8459    180.0   182.19\n",
      "  9030    138.0   132.67\n",
      " 12056    162.0   140.53\n",
      "  8414     82.0    96.69\n",
      " 10960    120.0    99.50\n",
      "  1844    116.0   151.78\n",
      " 10912     82.0   109.04\n",
      "  9176    150.0   170.50\n",
      "  5377    120.0   120.42\n",
      " 11794    108.0   120.63\n",
      "  7394    168.0   166.44\n",
      "  1939    138.0   143.12\n",
      " 15292    132.0   133.83\n",
      "  9682    168.0   178.83\n",
      " 13727    162.0   169.03\n",
      " 13146    120.0   140.09\n",
      "  7624     94.0    67.16\n",
      " 13637    156.0   142.28\n",
      "  1994     94.0   131.90\n",
      " 11289    120.0   122.37\n",
      "  2031    108.0   143.06\n",
      "  6826    132.0   144.35\n",
      " 12625     82.0    79.90\n",
      " 11035    144.0   132.67\n",
      "  2108    169.0   145.77\n",
      "  8393    192.0   202.25\n",
      " 12475     50.0    54.86\n",
      "  7012     88.0   107.80\n",
      "  6921    138.0   126.42\n",
      " 14859    180.0   179.09\n",
      " 14162    120.0   115.39\n",
      " 12283    126.0   130.69\n",
      "  3279    192.0   163.36\n",
      "  5371     72.0    79.67\n",
      " 11787    150.0   141.05\n",
      "  2582    132.0   110.96\n",
      " 10563     82.0    87.92\n",
      "  8320    120.0    89.77\n",
      " 15439    162.0   166.42\n",
      "  8602    126.0   141.73\n",
      " 15146    120.0   130.15\n",
      " 12286     94.0   122.12\n",
      " 11738    120.0   117.16\n",
      "  9720    156.0   159.03\n",
      "  8236    120.0   138.10\n",
      "  4611     96.0   111.72\n",
      " 11740    126.0   126.68\n",
      "  6848    138.0   153.29\n",
      " 10555    144.0   136.73\n",
      "  6055    144.0   151.11\n",
      " 13802    106.0   109.57\n",
      "  3714     54.0    80.79\n",
      " 10199    120.0   110.05\n",
      "  2742    138.0   124.31\n",
      "  5710    156.0   156.59\n",
      "  7520    168.0   178.17\n",
      "  9728    144.0   144.08\n",
      " 11346    150.0   161.63\n",
      "  5881    162.0   147.29\n",
      " 12364    120.0   121.56\n",
      "  9292    156.0   156.05\n",
      "  4823    192.0   189.21\n",
      " 15139     39.0    58.34\n",
      "  5594     60.0    47.24\n",
      "  4866    168.0   163.84\n",
      " 10140    120.0   132.86\n",
      "  1399     36.0    66.54\n",
      "  3175     94.0   110.39\n",
      " 10312     84.0    80.62\n",
      " 13935    150.0   132.84\n",
      "  5753     30.0    62.44\n",
      "  6288    150.0   138.01\n",
      " 10384     94.0   115.55\n",
      "  5796    144.0   152.73\n",
      "  3157    165.0    56.78\n",
      " 14998    150.0   118.79\n",
      "  8348     94.0   108.29\n",
      " 10624     69.0    80.24\n",
      "  7136    108.0   100.54\n",
      "  7344    156.0   146.36\n",
      " 13987    180.0   191.18\n",
      " 14537    180.0   181.08\n",
      " 11256    150.0   157.45\n",
      "  4228    138.0   136.70\n",
      "  2220    180.0   156.20\n",
      "  5294    156.0   152.16\n",
      "  5622    168.0   165.39\n",
      "  2928    162.0   165.45\n",
      " 14154    144.0   137.95\n",
      "  6660     60.0    88.24\n",
      "  2748    162.0   151.32\n",
      "  9299    156.0   153.31\n",
      " 13132    150.0   143.26\n",
      "  3146    165.0   141.75\n",
      "  2353     50.0    86.25\n",
      "  1943    204.0   183.06\n",
      "  8577    153.0   122.09\n",
      "  2674    180.0   186.17\n",
      "  9084    114.0    98.32\n",
      "  2045    135.0   138.70\n",
      " 12251    204.0   202.77\n",
      "  6621    162.0   166.15\n",
      "  6107    168.0   184.11\n",
      " 11662     76.0    79.70\n",
      "  4835     72.0    46.74\n",
      " 13456     72.0    66.20\n",
      "  4603    132.0   137.76\n",
      "  2544    168.0   181.19\n",
      "  5666    192.0   191.61\n",
      " 14261    144.0   158.34\n",
      "  3021    156.0   171.02\n",
      "  6985    150.0   162.86\n",
      "  1446    132.0   141.56\n",
      "  6439     96.0    85.76\n",
      "  8495    138.0   127.94\n",
      " 13005    138.0   141.29\n",
      "  9562    132.0   141.36\n",
      " 15164     60.0    80.19\n",
      " 14450    120.0   106.78\n",
      "  8433    132.0   128.34\n",
      "  6811     50.0    48.18\n",
      "  3736    100.0   102.95\n",
      "  7766    126.0   146.89\n",
      "  1457    126.0   137.92\n",
      "  9381    120.0   142.70\n",
      "  5984    156.0   136.47\n",
      " 14600    108.0   126.07\n",
      " 13280    168.0   171.23\n",
      " 11780    132.0   149.80\n",
      " 14532     82.0    66.06\n",
      "  3280    120.0   141.00\n",
      " 12740    144.0   128.00\n",
      "  9577     60.0    86.03\n",
      "  9417     32.0    45.08\n",
      "  6462    180.0   186.89\n",
      "  9952    162.0   168.92\n",
      "  4865    132.0   156.22\n",
      " 11605     82.0    85.13\n",
      "  8673    138.0   136.72\n",
      "  9414     94.0    87.54\n",
      "  5418    132.0   116.98\n",
      "  7647     42.0    46.79\n",
      "  6271    144.0   125.48\n",
      " 14711     82.0    85.81\n",
      "  6178    204.0   210.66\n",
      "  3506     96.0    94.02\n",
      " 14321    162.0   156.97\n",
      "  8478     82.0    92.57\n",
      "  8831    156.0   175.93\n",
      "  8708    162.0   162.08\n",
      "  1825    132.0   144.91\n",
      " 11388    156.0   161.96\n",
      " 15559    132.0   126.22\n",
      " 11731    180.0   187.30\n",
      "  4280    156.0   148.29\n",
      "  9313    150.0   157.36\n",
      " 14267    168.0   166.90\n",
      "  1807    159.0   155.46\n",
      "  5807     90.0    80.89\n",
      "  5186    144.0   134.72\n",
      " 11190    138.0   176.59\n",
      "  7062     82.0   102.21\n",
      " 14084    156.0   151.60\n",
      " 12268    156.0   160.08\n",
      " 10955    156.0   158.75\n",
      " 10498     54.0    54.80\n",
      " 14514     60.0    99.50\n",
      "  1743     64.0    82.65\n",
      "  6828    132.0   145.11\n",
      "  2078     82.0   106.84\n",
      "  4902     84.0    99.57\n",
      " 14653    100.0   104.89\n",
      "  9751    120.0   119.00\n",
      " 10824    204.0   193.52\n",
      "  9186     94.0    93.71\n",
      "  5465    156.0   157.75\n",
      "  9599    106.0   111.35\n",
      "  4890     84.0   107.97\n",
      "  5661     72.0    61.19\n",
      "  1841    102.0   138.00\n",
      " 13008     42.0    70.63\n",
      " 15382     60.0    66.68\n",
      "  7583    126.0   141.20\n",
      " 11709     94.0   133.68\n",
      "  8156     72.0    83.57\n",
      "  1592     39.0   110.23\n",
      "  2309    180.0   175.61\n",
      "  8920    144.0   145.01\n",
      " 13209    120.0   150.20\n",
      " 14769    144.0   169.44\n",
      "  2138    168.0   150.00\n",
      " 15194    168.0   162.02\n",
      " 14019    132.0   129.71\n",
      " 11730    106.0   141.85\n",
      " 14009    120.0   138.70\n",
      "  5472    156.0   155.92\n",
      "  4298     42.0    70.72\n",
      " 12808    120.0   122.38\n",
      "  6551     69.0    60.82\n",
      "  7950    168.0   172.22\n",
      "  3938    144.0   131.17\n",
      " 13908     57.0    56.21\n",
      " 12782     82.0    94.21\n",
      "  5380    204.0   210.61\n",
      " 12646     66.0    65.11\n",
      "  4759     72.0    71.00\n",
      "  9073    144.0   151.03\n",
      "  6442    156.0   152.67\n",
      "  4213    132.0   163.11\n",
      "  8551    132.0   118.67\n",
      "  2974     69.0    63.88\n",
      "  1452    180.0   145.36\n",
      " 12133     75.0    77.98\n",
      "  6214     82.0   104.50\n",
      "  8182    156.0   154.92\n",
      "  7397     78.0    68.41\n",
      " 10996    138.0   120.23\n",
      " 14822    156.0   169.40\n",
      "  1684     32.0    56.74\n",
      " 13434    168.0   169.21\n",
      "  5809    186.0   178.08\n",
      "  6561    150.0   134.46\n",
      "  4355    204.0   195.59\n",
      "  2354    180.0   193.84\n",
      "  5798    150.0   152.51\n",
      " 10211    132.0   149.14\n",
      " 10321     82.0   109.34\n",
      "  8477    120.0   120.01\n",
      "  8635    165.0   145.72\n",
      "  3214    196.0   188.27\n",
      "  3388     24.0    80.18\n",
      "  1570    150.0   143.81\n",
      "  7089    180.0   190.19\n",
      "  2188    168.0   161.65\n",
      "  5866     78.0    55.47\n",
      "  2938    180.0   177.76\n",
      "  8677     72.0    78.96\n",
      " 15193    106.0   117.82\n",
      " 10680    156.0   143.74\n",
      "  2168    146.0   126.44\n",
      "  3284    132.0   138.17\n",
      "  5552    168.0   148.12\n",
      "  7320    144.0   155.95\n",
      "  6007    156.0   150.63\n",
      " 11291    108.0    93.75\n",
      "  1838     66.0   128.02\n",
      "  8365    150.0   133.69\n",
      " 11857    168.0   155.75\n",
      "  7886     81.0    74.63\n",
      "  2456    180.0   182.15\n",
      "  3773    140.0   159.49\n",
      "  6923    120.0    99.07\n",
      " 11609    186.0   186.01\n",
      "  4608    156.0   148.56\n",
      " 11040    120.0   124.16\n",
      "  9090    144.0   130.10\n",
      "  8493    162.0   135.77\n",
      " 11415    165.0   167.02\n",
      "  3682    144.0   122.19\n",
      " 14376     39.0    44.95\n",
      "  7592    168.0   178.71\n",
      "  9735    114.0    83.45\n",
      "  1660    132.0   154.62\n",
      " 11536    159.0   144.34\n",
      "  5269    168.0   164.57\n",
      "  4147    180.0   167.45\n",
      " 14620     94.0   133.23\n",
      "  7996    108.0   111.50\n",
      " 12964    132.0   133.59\n",
      " 11682    138.0   155.13\n",
      " 14584    150.0   135.07\n",
      "  9828    138.0   153.47\n",
      "  4196    106.0   146.01\n",
      "  8676     94.0    95.33\n",
      "  2912    132.0   101.24\n",
      " 12278     96.0   103.17\n",
      "  3133     82.0   115.86\n",
      "  4973    108.0    83.21\n",
      " 13631    120.0    93.69\n",
      "  6737    120.0   122.64\n",
      "  5775    150.0   155.11\n",
      " 15414    165.0   165.36\n",
      "  9922     60.0    63.36\n",
      "  1597    168.0   164.53\n",
      "  7954    156.0   145.08\n",
      "  2577    204.0   181.61\n",
      " 13503     82.0   102.33\n",
      "  4719     96.0   117.27\n",
      " 15116    168.0   151.79\n",
      " 10383    162.0   162.06\n",
      "  2906    126.0   140.25\n",
      " 12376     60.0    57.55\n",
      " 13001    132.0   147.91\n",
      "  1880     72.0    99.84\n",
      " 11829    132.0   126.76\n",
      " 12387    132.0   101.61\n",
      " 14644     82.0   107.72\n",
      "  4822     48.0    74.30\n",
      "  3307    156.0   188.15\n",
      "  5869    150.0   132.97\n",
      "  4777    156.0   166.37\n",
      " 10727    115.0   152.10\n",
      "  9563    168.0   185.03\n",
      " 10414    132.0   135.25\n",
      "  5975    100.0   108.26\n",
      "  1426     54.0    86.10\n",
      " 14839    156.0   130.70\n",
      "  4963    180.0   178.59\n",
      "  1887    132.0   109.22\n",
      "  2349    192.0   143.78\n",
      " 13373     60.0    57.75\n",
      " 15347    156.0   147.66\n",
      " 13017    138.0   136.33\n",
      "  7374    180.0   177.73\n",
      " 11019    144.0   158.41\n",
      "  6504    156.0   171.61\n",
      " 10444    168.0   168.25\n",
      " 15410     94.0   107.17\n",
      "  5374     27.0    73.97\n",
      " 13668     60.0    59.64\n",
      "  1444    150.0   128.50\n",
      " 12532     94.0    86.19\n",
      " 14805    162.0   141.04\n",
      "  9397    180.0   196.29\n",
      "  5438     60.0    62.75\n",
      " 11262    192.0   194.21\n",
      " 12008    192.0   165.71\n",
      " 14265    162.0   146.77\n",
      " 10608     84.0    82.03\n",
      "  6652    132.0   132.46\n",
      "  7541    186.0   177.54\n",
      "  5786    162.0   188.79\n",
      "  7325    138.0   151.56\n",
      " 12118     60.0    57.95\n",
      " 11247     24.0    41.24\n",
      "  7651    162.0   160.44\n",
      "  3347    186.0   178.76\n",
      "  1972     82.0    70.55\n",
      " 10212    113.0    93.23\n",
      " 10301     84.0    65.17\n",
      " 14067    156.0   161.82\n",
      "  4185     54.0    59.72\n",
      " 10037     94.0    94.27\n",
      "  8147    120.0   155.17\n",
      "  2943    204.0   160.72\n",
      "  5548    150.0   144.36\n",
      "  1495    139.0   124.00\n",
      " 10014    120.0   128.11\n",
      "  8150    132.0   147.73\n",
      " 14866     69.0    61.64\n",
      " 14813     84.0    80.83\n",
      " 12535    144.0   155.02\n",
      "  3767    114.0   117.34\n",
      " 11640    132.0   135.43\n",
      "  3344    171.0   167.53\n",
      "  3828    156.0   164.64\n",
      " 14375    132.0   133.13\n",
      "  7981    168.0   173.68\n",
      "  3881    132.0   131.88\n",
      "  4885     96.0   102.27\n",
      "  7192    186.0   184.55\n",
      "  7475    120.0   143.26\n",
      "  9388    162.0   147.53\n",
      "  5141     42.0    74.72\n",
      "  9915    156.0   152.34\n",
      "  7636    138.0   146.80\n",
      "  7351    120.0   119.89\n",
      "  5200    156.0   159.03\n",
      "  5232    156.0   157.17\n",
      "  5772    114.0   138.81\n",
      "  3059    120.0   154.16\n",
      " 12765    174.0   176.90\n",
      " 12518     94.0   102.52\n",
      "  2414     36.0    26.15\n",
      "  6672    138.0   143.33\n",
      " 14343    204.0   191.94\n",
      "  8760    156.0   141.34\n",
      "  6275     72.0    85.81\n",
      "  5430     96.0    88.28\n",
      "  4753    102.0    99.77\n",
      "  6089    132.0   104.35\n",
      " 15246    168.0   174.56\n",
      "  4090     96.0    92.91\n",
      "  6374     96.0   118.60\n",
      " 11386    162.0   161.44\n",
      "  8251    156.0   154.41\n",
      " 10328    204.0   168.60\n",
      " 14569     96.0   106.21\n",
      "  5987    168.0   157.75\n",
      " 13933    138.0   137.13\n",
      " 12842    168.0   186.48\n",
      " 12891    150.0   130.92\n",
      "  9869    138.0   120.13\n",
      "  3097    150.0   140.44\n",
      "  8204    150.0   137.08\n",
      "  2965     32.0    44.40\n",
      "  9383    126.0   151.05\n",
      " 11207     82.0    92.40\n",
      " 14312     94.0   103.15\n",
      " 12038     69.0    83.52\n",
      "  9538    132.0   138.04\n",
      "  9000     78.0    68.22\n",
      "  5181    168.0   162.50\n",
      "  7450    150.0   148.36\n",
      " 11361     12.0    46.66\n",
      "  7026    120.0   138.66\n",
      "  9537     94.0   111.19\n",
      "  1848     96.0   109.75\n",
      "  9266     84.0    94.88\n",
      "  9529     50.0    74.60\n",
      " 11742    132.0   143.07\n",
      "  2367    180.0   180.26\n",
      "  5029     36.0    57.39\n",
      "  8767     96.0   105.98\n",
      " 14755     82.0    64.03\n",
      "  5868    106.0   110.73\n",
      " 15574    156.0   152.07\n",
      "  3718    150.0   137.43\n",
      "  1522     30.0    67.06\n",
      "  9503     72.0    82.40\n",
      " 10220    150.0   149.23\n",
      "  8336     72.0    86.12\n",
      " 10637    132.0   121.80\n",
      " 10734    156.0   157.98\n",
      " 11626    150.0   113.44\n",
      " 10782    204.0   156.21\n",
      "  8154    132.0   130.92\n",
      " 10750    120.0   120.79\n",
      " 10591     50.0    92.74\n",
      " 13045    138.0   138.31\n",
      " 13817     72.0    70.23\n",
      " 11828    168.0   170.44\n",
      " 11153    162.0   149.16\n",
      "  4177    198.0   176.97\n",
      "  5195    192.0   171.83\n",
      "  6738    162.0   160.37\n",
      "  7419     60.0    79.30\n",
      "  9724    180.0   172.32\n",
      "  2511    144.0   144.43\n",
      "  2129    156.0   171.95\n",
      " 13125     43.0    46.64\n",
      "  4937    132.0   130.40\n",
      " 12985     76.0    55.26\n",
      " 15025     82.0    74.39\n",
      "  4744    168.0   174.69\n",
      "  4333     90.0   121.97\n",
      "  2874     75.0   119.34\n",
      "  3579    152.0   164.36\n",
      " 12517     48.0    45.29\n",
      " 11311    132.0   146.19\n",
      " 11214    126.0   124.04\n",
      "  6588     82.0    84.55\n",
      "  1398      4.0    15.14\n",
      "  8426    156.0   153.10\n",
      "  9154    144.0   152.02\n",
      "  6902    120.0   140.70\n",
      "  3701    138.0   141.57\n",
      "  8658    168.0   164.12\n",
      "  5709    168.0   165.88\n",
      "  1534    108.0   129.57\n",
      " 15522     82.0    84.21\n",
      "  6791    156.0   156.48\n",
      " 10961     94.0   102.25\n",
      "  1526     96.0   102.49\n",
      "  8451    120.0   137.45\n",
      "  9212    120.0   116.17\n",
      "  4639    204.0   174.81\n",
      "  2373    156.0   177.19\n",
      " 14647    180.0   200.24\n",
      "  9584     18.0   134.30\n",
      " 11180    156.0   111.37\n",
      "  5009    144.0   146.47\n",
      "  1772     48.0    76.72\n",
      "  8225     36.0    39.97\n",
      "  3995    144.0   136.98\n",
      " 10855    192.0   190.02\n",
      "  3401    168.0   183.46\n",
      "  5806    138.0   143.20\n",
      "  3137    144.0   145.13\n",
      " 14248    120.0   126.73\n",
      "  2596     54.0    55.04\n",
      "  7056    156.0   157.43\n",
      "  7937     60.0    57.14\n",
      " 11405    132.0   125.70\n",
      "  2876    120.0   137.27\n",
      " 15445    204.0   207.21\n",
      " 10535    174.0   173.33\n",
      " 11832    108.0   122.47\n",
      " 10789    132.0   138.28\n",
      "  3678    100.0   166.26\n",
      "  6925     94.0    95.98\n",
      " 10731    132.0   114.36\n",
      " 14719    159.0   155.60\n",
      "  3283    168.0   180.54\n",
      " 12272    108.0   127.74\n",
      "  5126    144.0   159.39\n",
      "  1929    120.0    86.49\n",
      " 10815    162.0   154.29\n",
      " 10705    108.0   100.77\n",
      "  6756    180.0   194.05\n",
      "  6051    162.0   178.80\n",
      "  1540    132.0   137.81\n",
      "  8914    186.0   176.44\n",
      " 14195     66.0    70.20\n",
      "  8915    156.0   156.83\n",
      "  7099     60.0    86.48\n",
      " 12074     57.0    36.10\n",
      "  2817     94.0    94.83\n",
      " 11530    120.0   118.48\n",
      "  5089     94.0   121.30\n",
      " 11786    132.0   128.20\n",
      "  3905     54.0    55.23\n",
      " 14494     69.0   100.28\n",
      " 12267    150.0   152.59\n",
      " 10225    168.0   159.73\n",
      "  4769    106.0   101.85\n",
      "  1574    132.0   126.92\n",
      "  6397    192.0   199.88\n",
      "  6679    162.0   140.79\n",
      " 14093     36.0    64.94\n",
      " 12895    168.0   166.52\n",
      " 11615    144.0   157.88\n",
      "  4337    100.0   149.43\n",
      "  6232    162.0   139.52\n",
      "  9750     50.0    56.35\n",
      " 13260    156.0   148.74\n",
      "  4059     60.0    42.56\n",
      "  7161     94.0   120.79\n",
      " 11038    106.0   122.21\n",
      "  4988    156.0   151.62\n",
      "  7339    138.0   130.79\n",
      " 10318    106.0   123.87\n",
      " 11181    132.0   137.06\n",
      " 12731    168.0   166.84\n",
      "  9716    168.0   150.08\n",
      "  3497     18.0    54.94\n",
      "  9013    204.0   184.83\n",
      " 12970    180.0   188.13\n",
      "  1474    126.0   158.19\n",
      " 10240    144.0   156.25\n",
      "  2909    162.0   139.44\n",
      " 13714    168.0   170.35\n",
      "  9348    150.0   150.27\n",
      " 14117     82.0    98.12\n",
      "  9820     84.0    67.62\n",
      " 12237    162.0   159.35\n",
      "  5845    192.0   182.93\n",
      " 15016    132.0   133.78\n",
      " 10859    156.0   151.48\n",
      " 15482    106.0   132.94\n",
      " 11844    132.0   158.61\n",
      "  3258    132.0   162.15\n",
      " 12888     82.0    82.60\n",
      " 10504    132.0   134.38\n",
      " 10918    120.0   152.98\n",
      "  3368    126.0   165.03\n",
      "  9440    162.0   171.48\n",
      " 13105     84.0    94.32\n",
      "  8099    162.0   155.65\n",
      "  3197    159.0   173.67\n",
      "  2366    168.0   192.23\n",
      " 14631    168.0   153.84\n",
      "  2795    120.0   118.27\n",
      " 12479    150.0   135.04\n",
      "\n",
      " Validation Loss: 13.5307\n",
      "Mean Absolute Error: 13.53 months\n",
      "Submission file saved!\n"
     ]
    }
   ],
   "source": [
    "model = BoneAgeModel()\n",
    "model = train_model(model, training_dataloader, testing_dataloader, num_epochs=20)\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "epoch_val_loss = 0\n",
    "mae_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, ages, _,img_ids in testing_dataloader:\n",
    "        images = images.float()\n",
    "        ages = ages.float()\n",
    "\n",
    "        predictions = model(images).squeeze()\n",
    "\n",
    "        for i in range(len(img_ids)):\n",
    "            test_predictions.append([img_ids[i].item(), ages[i].item(), predictions[i].item()])\n",
    "            loss = torch.abs(predictions[i] - ages[i])\n",
    "            epoch_val_loss += loss.item()\n",
    "            mae_total += loss.item()\n",
    "\n",
    "            print(f\"{img_ids[i].item():>6} {ages[i].item():>8} {predictions[i].item():>8.2f}\")\n",
    "\n",
    "val_loss = epoch_val_loss / len(testing_dataloader.dataset)\n",
    "mae_months = mae_total / len(testing_dataloader.dataset)\n",
    "print(f\"\\n Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae_months:.2f} months\")\n",
    "\n",
    "submission_df = pd.DataFrame(test_predictions, columns=[\"id\", \"real\", \"prediction\"])  \n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_losses, val_losses, val_maes):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Losses plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # MAE plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_maes, label='Validation MAE', marker='o', color='g')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE (months)')\n",
    "    plt.title('Validation MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.530732584075203\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "submission = pd.read_csv('submission.csv')\n",
    "submission['difference'] = submission['real'] - submission['prediction']\n",
    "submission['difference'] = abs(submission['difference'])\n",
    "\n",
    "total = submission['difference'].sum()\n",
    "error = total / 631\n",
    "print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
